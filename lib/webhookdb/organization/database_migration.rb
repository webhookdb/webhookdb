# frozen_string_literal: true

class Webhookdb::Organization::DatabaseMigration < Webhookdb::Postgres::Model(:organization_database_migrations)
  include Webhookdb::Dbutil

  class MigrationInProgress < Webhookdb::DatabaseLocked; end
  class MigrationAlreadyFinished < StandardError; end

  plugin :timestamps
  plugin :text_searchable, terms: [:organization, :started_by]
  plugin :column_encryption do |enc|
    enc.column :source_admin_connection_url
    enc.column :destination_admin_connection_url
  end

  many_to_one :started_by, class: "Webhookdb::Customer"
  many_to_one :organization, class: "Webhookdb::Organization"
  many_to_one :last_migrated_service_integration, class: "Webhookdb::ServiceIntegration"

  dataset_module do
    def ongoing
      return self.where(finished_at: nil)
    end
  end

  def self.guard_ongoing!(org)
    dbm = self.where(organization: org).ongoing.first
    return if dbm.nil?
    raise MigrationInProgress, "Organization #{org.name} has an ongoing database host migration so " \
                               "cannot be modified. We'll let admins know when it's done. Try again soon."
  end

  def self.enqueue(admin_connection_url_raw:, readonly_connection_url_raw:, public_host:, started_by:, organization:)
    self.guard_ongoing!(organization)
    self.db.transaction do
      dbm = self.create(
        started_by:,
        organization:,
        organization_schema: organization.replication_schema,
        source_admin_connection_url: organization.admin_connection_url_raw,
        destination_admin_connection_url: admin_connection_url_raw,
      )
      organization.update(
        public_host:,
        admin_connection_url_raw:,
        readonly_connection_url_raw:,
      )
      return dbm
    end
  end

  def displaysafe_source_url
    return displaysafe_url(self.source_admin_connection_url)
  end

  def displaysafe_destination_url
    return displaysafe_url(self.destination_admin_connection_url)
  end

  def status
    return "finished" if self.finished_at.present?
    return "in_progress" if self.started_at.present?
    return "enqueued"
  end

  def finished?
    return !!self.finished_at
  end

  def migrate
    raise MigrationAlreadyFinished if self.finished?
    self.update(started_at: Time.now) if self.started_at.nil?
    borrow_conn(self.source_admin_connection_url) do |srcdb|
      borrow_conn(self.destination_admin_connection_url) do |dstdb|
        self.organization.service_integrations.sort_by(&:id).each do |sint|
          next if sint.id <= self.last_migrated_service_integration_id
          self.migrate_service_integration(sint, srcdb, dstdb)
          self.update(last_migrated_service_integration_id: sint.id, last_migrated_timestamp: nil)
        end
      end
    end
    self.update(finished_at: Time.now)
  end

  # @param [Webhookdb::ServiceIntegration] service_integration
  protected def migrate_service_integration(service_integration, srcdb, dstdb)
    svc = service_integration.replicator
    # If the service integration was not synced in the old db, skip it
    return unless srcdb.table_exists?(svc.qualified_table_sequel_identifier)
    svc.create_table_modification(if_not_exists: true).execute(dstdb)
    ds = srcdb[svc.qualified_table_sequel_identifier].order(svc.timestamp_column.name)
    (ds = ds.where(Sequel[svc.timestamp_column.name] > self.last_migrated_timestamp)) unless
      self.last_migrated_timestamp.nil?
    chunksize = Webhookdb::Organization.database_migration_page_size
    chunk = []
    ds.paged_each(rows_per_fetch: chunksize, hold: true, cursor_name: "whdb_dbmigration_#{self.id}") do |row|
      chunk << row
      if chunk.size >= chunksize
        self.upsert_chunk(service_integration, dstdb, chunk)
        chunk.clear
        Amigo::DurableJob.heartbeat
      end
    end
    self.upsert_chunk(service_integration, dstdb, chunk)
  end

  # @param [Webhookdb::ServiceIntegration] service_integration
  protected def upsert_chunk(service_integration, dstdb, chunk)
    return if chunk.empty?
    svc = service_integration.replicator
    chunk.each { |h| h.delete(svc.primary_key_column.name) }
    tscol = svc.timestamp_column.name
    dstdb[svc.qualified_table_sequel_identifier].
      insert_conflict(
        target: svc.remote_key_column.name,
        update_where: svc._update_where_expr,
      ).multi_insert(chunk)
    self.update(last_migrated_timestamp: chunk.last[tscol])
  end

  def finish(now: Time.now)
    self.update(
      finished_at: now,
      source_admin_connection_url: displaysafe_source_url,
      destination_admin_connection_url: displaysafe_destination_url,
    )
    return self
  end
end

# Table: organization_database_migrations
# -------------------------------------------------------------------------------------------------------------------------
# Columns:
#  id                                   | integer                  | PRIMARY KEY GENERATED BY DEFAULT AS IDENTITY
#  created_at                           | timestamp with time zone | NOT NULL DEFAULT now()
#  updated_at                           | timestamp with time zone |
#  started_at                           | timestamp with time zone |
#  finished_at                          | timestamp with time zone |
#  organization_id                      | integer                  | NOT NULL
#  started_by_id                        | integer                  |
#  source_admin_connection_url          | text                     |
#  destination_admin_connection_url     | text                     |
#  organization_schema                  | text                     |
#  last_migrated_service_integration_id | integer                  | NOT NULL DEFAULT 0
#  last_migrated_timestamp              | timestamp with time zone |
#  text_search                          | tsvector                 |
# Indexes:
#  organization_database_migrations_pkey                  | PRIMARY KEY btree (id)
#  one_inprogress_migration_per_org                       | UNIQUE btree (organization_id) WHERE finished_at IS NULL
#  organization_database_migrations_organization_id_index | btree (organization_id)
# Foreign key constraints:
#  organization_database_migrations_organization_id_fkey | (organization_id) REFERENCES organizations(id) ON DELETE CASCADE
#  organization_database_migrations_started_by_id_fkey   | (started_by_id) REFERENCES customers(id) ON DELETE SET NULL
# -------------------------------------------------------------------------------------------------------------------------
